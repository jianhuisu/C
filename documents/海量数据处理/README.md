## 背景分析

为了迅速占领市场,需要精准分析用户群体行为提供决策支持.数以亿计的用户产生海量的数据.
为了对海量的数据进行分析,单单依靠对计算机的硬件进行扩容显的捉襟见肘.利用现有条件进行海量数据信息处理成为热点面试问题.

何谓海量，就是数据量太大，所以导致要么是无法在较短时间内迅速解决.要么是数据太大，导致**无法一次性装入内存**。

其中知识点主要聚焦在以下三种问题上

 - 海量数据中的热点数据(Top K):海量数据中找出出现频率最高的前K个数.
 - 海量数据的排序:一个文件中有九亿条不重复的9位整数,对这个文件中的数字进行排序.
 - 海量数据的重复问题：在海量数据中查找重复出现的元素或者去除重复出现的元素.

针对以上三种问题,不同的场景下主要有以下常用解决方案.(根据实用程度)

 - 将数据转存到数据库.借助数据库工具,通过操控SQL语句来完成.(`select distinct` `count(1) order by`).
 - 资源分配允许情况下,增大进程使用内存资源限额.(目的是使待处理数据可以一次性装入内存.不过面试时建议答这条,该适合偷懒).
 - Hash法
 - Bloom filter法(布隆过滤器)
 - 倒排索引法:搜索引擎常用
 - 外部排序(归并排序...) B树 B+ B_
 - Trie树(字典树/键树),是一种多叉树.
 - 堆排序 
 - MapReduce法 
 
#### 解决方案方法论

核心思路：大而化小，分而治之.

#### 经典实例分析

每个焦点问题一个实例.

##### `top K`

百度面试题：

搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为`1-255`字节。
假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。
一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。），请你统计最热门的10个查询串，要求使用的内存不能超过1G。

分析:

    一个查询串的长度位`255B`,我们算`256`B(约等于`2^8`B).
        假设 2^10 约等于1000,一千万个记录大小是 
            
            1000 * 10^4 * 2^8 
            1000 * 1000 * 10 * 2^8 
            约等 2^10 * 2^10 * 2^3 * 2^8 = 2^31  
            1GB = 2^30
            
        2^31 > 2^30 
        
所以一千万的数据无法一次性装入内存.文件比较大，无法一次性读入内存，可以采用`hash取模`的方法，将大文件分解为多个小文件.
假设去重后300万的数据可以一次性装入内存,可以采用`hash_map`来统计出重复次数.然后选择合适的内部排序方法直接排序,选出`Top K`.
(我们要学会估算数据大概占用多少空间).

 1. 维护一个`key`为`query字串`，`value`为该`query字串`出现次数的`hashTable`.
 2. 每次读取一个`query`，如果该`query`不在`hashtable`中，那么插入该`query串`，并且将`value`值设为1.
 3. 如果该字串在`hashtable`中，那么将`query串`对应值的计数加一.最终我们在`O(n)`的时间复杂度内用`hashtable`表完成了统计重复次数；
 4. 现在`hashtable`表中存储有`300万`条记录.对三百条记录进行内部排序.我们可以使用`堆排序`对这三百万条记录进行进行排序.借助堆这个数据结构，找出`Top K`，时间复杂度为`(N*logK)`.

**这里的`堆排序`的使用是非常有技巧的**.借助堆结构，我们可以在`O(nlogn)`量级的时间内查找和调整/移动.
所以,维护一个`K`(**该题目中是10**)大小的小根堆(我们要局部淘汰最小值,小根堆的根结点值最小).
然后遍历300万的`query`次数，分别和根元素进行对比.如果,大于根结点的值,那么将新值加入堆中.重新构建小根堆.然后继续对比.
最终，我们最终的时间复杂度是：

    O（N） + N' * O（KlogK），（N为1000万，N’为300万）.

此方法的巧妙之处在于堆中，查找等各项操作时间复杂度均为`logk`.

MapReduce框架特别适合解决`Top K`问题.

 1. Map(分发 处理):首先根据数据值把数据hash后的值 按照范围划分到不同的机器上,理想情况下划分完的数据可以一次性读入内存
 2. 各个机器使用内部排序计算出各自出现次数最多的前N个数据.
 3. Reduce(汇总 提炼):多各个机器的结果进行汇总,选出所有的数据中出现次数最多的N个数据.

**注意:直接将数据均匀的分摊到不同的机器上进行处理是无法得到正确结果的.只有一个数据完全聚集到一个机器上,最后的汇总结果才会正确.
所以在Map过程中,我们需要采用hash算法决定分配.在汇总阶段,我们可以借助堆接口,快速完成汇总.**

总结来说就是`分而治之/Hash映射 + Hash_map统计 + 堆/快速/归并排序`
      
###### Top K问题的变种

 1. 10个文件,每个文件1GB 每个文件中的每一行都存储一条query,每个文件中的query都可能重复.请按照query的频度排序.
 1. 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。（**分成多少份 1G / 1M = 1000 份**）
 1. 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。
 1. 如何从海量访问日志中提取访问次数最多的IP地址 采用hash取模保证一种ip只出现在一个子文件中.https://blog.csdn.net/tayanxunhua/article/details/20528389
 1. 10亿个整数找出重复次数最多的100个整数. 
 1. 搜索的输入信息是一个字符串,统计300万条输入信息中最热门的前10条.每次输入的一个字符串不超过255B，内存使用只有1GB.
 1. 有1000万个身份证号以及他们对应的数据,身份证号可能重复.找出出现次数最多的身份证号.
 1. 100w个数中找出最大的100个数。
 1. 上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据   
 1. 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？
 
    分而治之/hash映射:
    遍历文件a，对每个url求取，然后根据所取得的值将url分别存储到1000个小文件中。这样每个小文件的大约为300M。
    遍历文件b，采取和a相同的方式将url分别存储到1000小文件中。这样处理后，所有可能相同的url都在对应的小文件中，不对应的小文件不可能有相同的url。   
    然后我们只要对比1000对小文件中相同的url即可。
    
 1. 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。（hash_map,sort,归并）
 1. 100w个数中找出最大的100个数。(用一个含100个元素的最小堆完成。复杂度为`O(100w * 100* lg100)`，或者 局部淘汰法)

#### 重复问题

问题:

在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。

分析:

使用`bit-map`缩小数据规模,然后去重.
采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。
然后扫描这2.5亿个整数，查看`Bitmap`中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。

#### 海量数据排序

问题:

有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。
     
方案1.

 1. hash映射：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为a0,a1,..a9）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。
 2. hash_map统计：找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。
 3. 堆/快速/归并排序：利用快速/堆/归并排序按照出现次数进行排序，将排序好的query和对应的query_cout输出到文件中，
 4. 这样得到了10个排好序的文件（记为）。最后，对这10个文件进行归并排序（内排序与外排序相结合）

方案2.

一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。
这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。
 

#### 参考资料

版权声明：本文为CSDN博主「v_JULY_v」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/v_july_v/article/details/7382693
    